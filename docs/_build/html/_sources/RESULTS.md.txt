# Results and Reporting

This page provides templates for recording results after running experiments. Populate the tables using artifacts produced by the runner and benchmarks. Statistical significance tests are available in [dch_pipeline/stats.py](../dch_pipeline/stats.py).

Notes:
- Artifacts directory (default): `./artifacts/<exp>*/`
  - Metrics CSV: `metrics.csv`
  - JSONL logs: `metrics.jsonl`
  - Merged config: `config.merged.json`
- Benchmarks print a single JSON line to stdout for easy capture.

---

## DVS Gesture

Enter results aggregated over seeds/runs.

| Setting | Accuracy (%) | Macro F1 | Micro F1 | Latency (ms/step) | Throughput (events/s) | Notes |
|---:|---:|---:|---:|---:|---:|---|
| DCH-only (torch-free) |  |  |  |  |  |  |
| DCH + SNN (Norse LIF) |  |  |  |  |  |  |

Ablations:
- DHG params (k, combination_order_max, δ_causal)
- Traversal (horizon, beam_size)
- Plasticity (ema_alpha, decay_lambda, prune_threshold)
- FSM (theta, lambda_decay, hold_k, min_weight)
- Abstraction enabled vs disabled

Significance (paired tests over seeds):
- Use utilities from [dch_pipeline/stats.py](../dch_pipeline/stats.py): `paired_ttest`, `wilcoxon_signed_rank`, `cohens_d_paired`, `cliffs_delta`, and `benjamini_hochberg` for multiple comparisons.
- Recommended report: mean ± std across seeds; p-values (post-correction) per comparison.

---

## N-MNIST

Enter results aggregated over seeds/runs.

| Setting | Accuracy (%) | Macro F1 | Micro F1 | Latency (ms/step) | Throughput (events/s) | Notes |
|---:|---:|---:|---:|---:|---:|---|
| DCH-only (torch-free) |  |  |  |  |  |  |
| DCH + SNN (Norse LIF) |  |  |  |  |  |  |

Ablations and significance:
- Mirror DVS Gesture ablations; prefer identical seeds for paired testing.
- Apply multiple-hypothesis correction with `benjamini_hochberg` on the p-value list.

---

## How to run (deterministic, offline-safe)

The following commands produce small, reproducible artifacts without external downloads. Each emits a single JSONL line with compact fields suitable for CI and quick inspection.

1) Quick synthetic run
- Command:
  ```bash
  python -m scripts.run_quick_experiment --mode synthetic --artifacts-dir artifacts/quick
  ```
  - Entry point: [run_main()](scripts/run_quick_experiment.py:36)
  - Output: artifacts/quick/metrics.jsonl with keys at least:
    - config_fingerprint (stable SHA-256 over config spec)
    - env (environment fingerprint)
    - metrics (deterministic counters)
    - reliability_summary (Beta posterior mean and MC CI per-edge)
  - Minimal JSON excerpt (illustrative):
    ```json
    {
      "config_fingerprint": "3f786850e387...d3d3",
      "env": {"python_version": "...", "platform": "..."},
      "metrics": {"n_admitted": 1, "n_grow": 1},
      "reliability_summary": {
        "mean_reliability": 0.55,
        "edges": [{"id": "2@10000<=1@9950", "mean": 0.57, "ci": [0.48, 0.66]}]
      }
    }
    ```
  - Internals: reliability summary uses [summarize_reliability()](dch_pipeline/evaluation.py:483) and fixed-seed [credible_interval_mc()](dch_core/beta_utils.py:150).

2) Dataset micro mode (offline-safe)
- Command (uses micro config fallback to avoid downloads):
  ```bash
  python -m scripts.run_dataset_experiment \
    --dataset nmnist \
    --config-path configs/micro.yaml \
    --limit 10 \
    --artifacts-dir artifacts/dataset_micro
  ```
  - Entry point: [run_main()](scripts/run_dataset_experiment.py:40)
  - Output schema matches the synthetic runner (JSONL with dataset and config_fingerprint).

3) Ablations (Beta vs EMA; abstraction flags)
- Command:
  ```bash
  python -m scripts.run_ablation_suite \
    --artifacts-root artifacts/ablations \
    --replicates 2 \
    --plasticity beta ema \
    --abstraction 0 \
    --align-by name
  ```
  - Entry point: [run_main()](scripts/run_ablation_suite.py:22)
  - Condition labels: p=<impl>|abs=<0/1>, e.g., p=beta|abs=0
  - The single-line JSON report contains:
    - conditions: label -> {n, artifacts[]}
    - aggregate: group-level reliability means and time series
    - comparisons: effect sizes via [effect_size_time_series()](dch_pipeline/evaluation.py:1025)

4) Baselines (offline-safe with dry-run fallback)
- Command:
  ```bash
  python -m scripts.run_baselines_experiment \
    --baseline norse_sg \
    --artifacts-dir artifacts/baselines \
    --limit 5
  ```
  - Entry point: [run_main()](scripts/run_baselines_experiment.py:98)
  - Behavior: if optional deps are missing, falls back to dry-run metrics; artifact schema remains compatible.

5) Statistical report
- Command:
  ```bash
  python -m scripts.run_stats_report \
    --a artifacts/quick \
    --a artifacts/dataset_micro \
    --align-by name \
    --alpha 0.05 \
    --output reports/stats_report.jsonl
  ```
  - Entry point: [run_main()](scripts/run_stats_report.py:75)
  - Computes reliability CI time series via [reliability_ci_time_series()](dch_pipeline/evaluation.py:961) and optional effect sizes.

Notes
- Use configs/micro.yaml examples to stay offline-safe.
- Artifacts are single-line JSONL for deterministic parsing.
