# Reproducibility Guide — Dynamic Causal Hypergraph (DCH)

Status: Draft v0.1  
Date: 2025-10-04  
Owners: DCH Maintainers  
License: MIT

Purpose
- This guide documents how to reproduce all experiments and artifacts for the DCH submission (NeurIPS 2025 target). It specifies environments, seeds, dataset handling, configuration management, logging, and archival requirements.

Authoritative references
- Decision record: [docs/FrameworkDecision.md](docs/FrameworkDecision.md)
- Packaging plan: [docs/PaperPackaging.md](docs/PaperPackaging.md)
- Interfaces: [docs/Interfaces.md](docs/Interfaces.md), [dch_core/interfaces.py](dch_core/interfaces.py)
- Algorithms: [docs/AlgorithmSpecs.md](docs/AlgorithmSpecs.md)
- Evaluation protocol: [docs/EVALUATION_PROTOCOL.md](docs/EVALUATION_PROTOCOL.md)
- CI and containers: [.github/workflows/ci.yml](.github/workflows/ci.yml), [Dockerfile](Dockerfile), [scripts/with_docker.sh](scripts/with_docker.sh)
- Runtime/Dev env: [pyproject.toml](pyproject.toml), [requirements.txt](requirements.txt), [environment.yml](environment.yml)

Scope and claims
- We provide CPU-only reproducibility out of the box. CUDA is optional (containers provided).
- We fix all random seeds for each run and report mean ± 95% CI across multiple seeds/splits.
- We archive all artifacts (configs, metrics, environment fingerprints, checkpoints) with provenance.

Environments

1) Python virtualenv (CPU)
- Create and activate:
  - `python3 -m venv .venv && source .venv/bin/activate`
- Install:
  - `pip install -U pip wheel`
  - `pip install -e .`
  - Optionally dev tools: `pip install -r requirements.txt`
- Verify:
  - `python -c "import torch, norse; print(torch.__version__)"`

2) Conda (CPU)
- Create:
  - `conda env create -f environment.yml`
- Activate:
  - `conda activate dch`
- Verify:
  - `python -c "import torch, norse; print(torch.__version__)"`

3) Docker (CPU and CUDA)
- Build CPU:
  - `bash scripts/with_docker.sh build cpu`
- Run CPU shell:
  - `bash scripts/with_docker.sh run cpu -- bash`
- Build CUDA:
  - `bash scripts/with_docker.sh build cuda`
- Run CUDA shell:
  - `bash scripts/with_docker.sh run cuda -- bash`
- Notes:
  - CUDA workflow requires NVIDIA drivers and nvidia-container-toolkit.

Dataset acquisition and layout
- Use the dataset downloader (to be implemented) [scripts/download_datasets.py](scripts/download_datasets.py) or manual instructions in [docs/EVALUATION_PROTOCOL.md](docs/EVALUATION_PROTOCOL.md).
- Directory structure:
  - `./data/dvs_gesture/...`
  - `./data/nmnist/...`
- Verify checksums when available and record dataset versions (URIs/hashes) in environment fingerprint.

Configuration management
- Hydra-based configuration files (to be added):
  - [configs/pipeline.yaml](configs/pipeline.yaml) core pipeline
  - [configs/dch.yaml](configs/dch.yaml) DCH hypergraph parameters
  - [configs/fsm.yaml](configs/fsm.yaml) FSM settings
  - [configs/scaffolding.yaml](configs/scaffolding.yaml) task-aware policies
  - [configs/model/norse_lif.yaml](configs/model/norse_lif.yaml) Norse model defaults
  - [configs/experiments/dvs_gesture.yaml](configs/experiments/dvs_gesture.yaml)
  - [configs/experiments/nmnist.yaml](configs/experiments/nmnist.yaml)
  - [configs/cv.yaml](configs/cv.yaml) cross-validation
  - [configs/hyperparams_sweep.yaml](configs/hyperparams_sweep.yaml) sweeps
- Each run saves an immutable config snapshot under `artifacts/<run_id>/config.yaml`.

Randomness and determinism
- Seeds are centrally controlled via [set_global_seeds()](dch_pipeline/replay.py:50):
  - Applies Python random, NumPy (if available), and torch (if available) seeds deterministically.
  - Optional torch flags (when applicable): `torch.use_deterministic_algorithms(True)`, `torch.backends.cudnn.deterministic=True`, `torch.backends.cudnn.benchmark=False`.
- For ablations and baselines, use multiple seeds (e.g., 2–5) and report mean ± 95% CI; quick runners are single-seed deterministic.
- Seeds and environment are captured in quick-run artifacts (see "env" field).

Environment fingerprinting
- Use [get_environment_fingerprint()](dch_pipeline/replay.py:113) to collect:
  - python_version (sys.version), platform string
  - torch version, CUDA, cuDNN if available
  - discovered package versions (numpy/torch)
- The quick synthetic runner embeds this as `env` in metrics.jsonl.

Logging and artifacts
- EAT logger (tamper-evident JSONL with hash chaining): [EATAuditLogger](dch_pipeline/eat_logger.py:79). Integrity can be checked offline via [verify_file()](dch_pipeline/eat_logger.py:280).
- CRC extraction (optional, opt-in): [CRCLogger](dch_pipeline/crc_logger.py:38) emits one JSON line per rule card.
- Quick synthetic: [run_main()](scripts/run_quick_experiment.py:36) → artifacts/<run>/metrics.jsonl
- Dataset micro: [run_main()](scripts/run_dataset_experiment.py:40) → artifacts/<run>/metrics.jsonl
- Statistics: reports via [run_main()](scripts/run_stats_report.py:75)

Baseline comparability
- Baselines should reuse the same preprocessing/encoding pipeline where applicable:
  - Norse surrogate-gradient: [baselines/norse_sg.py](baselines/norse_sg.py)
  - BindsNET STDP: [baselines/bindsnet_stdp.py](baselines/bindsnet_stdp.py)
- Ensure batch sizes, number of epochs, and evaluation metrics align with DCH runs.
- Report baseline configs, seeds, and environment fingerprint.

Statistical testing and reporting
- Implement statistical procedures in [dch_pipeline/stats.py](dch_pipeline/stats.py) to compute:
  - Paired t-test, Wilcoxon signed-rank.
  - Effect sizes: Cohen’s d (paired), Cliff’s delta.
  - Multiple comparison control: Benjamini–Hochberg FDR (q=0.05).
- Report:
  - Means ± 95% CI across seeds (and folds for CV).
  - p-values and effect sizes for DCH vs baselines.
  - Number of runs, total compute time per configuration.

Compute fairness and budgets
- Report wall-clock, CPU/GPU utilization (if available), and hardware specs.
- Keep hyperparameter search budgets comparable across methods; document search grids.

Step-by-step reproduction (CPU quickstart)
1) Create environment:
   - `python3 -m venv .venv && source .venv/bin/activate`
   - `pip install -U pip wheel && pip install -e . && pip install -r requirements.txt`
2) (Optional) Acquire datasets:
   - For offline-safe micro runs, use configs/micro.yaml and synthetic fallbacks (no downloads).
3) Run tests:
   - `pytest -q`
4) Quick synthetic artifact:
   - `python -m scripts.run_quick_experiment --mode synthetic --artifacts-dir artifacts/quick`
   - Produces artifacts/quick/metrics.jsonl with `config_fingerprint`, `env`, `metrics`, `reliability_summary`
5) Dataset micro artifact:
   - `python -m scripts.run_dataset_experiment --dataset nmnist --config-path configs/micro.yaml --limit 10 --artifacts-dir artifacts/dataset_micro`
6) Statistical report (combine artifacts):
   - `python -m scripts.run_stats_report --a artifacts/quick --a artifacts/dataset_micro --align-by name --alpha 0.05 --output reports/stats_report.jsonl`

Archival for submission
- Prepare a release under [docs/export/](docs/export/):
  - Source archive (matching tag).
  - Docker images sha256 digests and Dockerfiles.
  - Environment files (`requirements.txt`, `environment.yml`).
  - Final result tables (CSV/JSON) with generation scripts and commit hash.
  - Paper PDF and supplementary (notebooks, figures).
- Tag release v1.0.0 for camera-ready.

Governance and citation
- License: MIT (to be added) [LICENSE](LICENSE)
- Citation metadata: [CITATION.cff](CITATION.cff)
- Code of Conduct: [docs/CODE_OF_CONDUCT.md](docs/CODE_OF_CONDUCT.md)
- Contributing guide: [docs/CONTRIBUTING.md](docs/CONTRIBUTING.md)

Known limitations and notes
- Some nondeterminism may persist on GPU even with deterministic flags; CPU runs provide the reference.
- Baselines may require slight deviations for fairness (document any departures).
- Large logs/snapshots can be disabled for speed; ensure the final runs used for the paper re-enable required logs.

Change control
- Any modification to evaluation or reproduction steps requires:
  - Updating this document with a changelog entry.
  - Bumping an experiment version id in configs and result tables.

Appendix A — Checklist
- [ ] Environment created and verified (Python/Conda or Docker)
- [ ] Datasets downloaded and checksums verified (if available)
- [ ] Seeds fixed and recorded
- [ ] Configs snapshot saved per run
- [ ] Metrics, TB logs, and env fingerprints archived
- [ ] Results table reproduced with CI log and artifact hashes
- [ ] Release tagged and artifacts published to docs/export/

End of guide